import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import os
import warnings
from typing import Tuple, List, Dict, Optional
from dataclasses import dataclass
from scipy.interpolate import interp1d

warnings.filterwarnings("ignore")

plt.rcParams["font.family"] = ["DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False

BASE_DIR = "."
INPUT_FILE = "calculation.xlsx"
DATA_FILE = "data.xlsx"
PREDICTION_CONDITIONS_FILE = "prediction_conditions.xlsx"

OUTPUT_PROCESSING_RESULTS = "processing_results_GPU.xlsx"
OUTPUT_GA_RESULTS = "dynamic_recrystallization_GPU_results.xlsx"
OUTPUT_GA_PLOT = "dynamic_recrystallization_GPU_fitting.png"
OUTPUT_PREDICTION_RESULTS = "GPU_predictions.xlsx"
OUTPUT_INPUT_PREDICTIONS = "GPU_input_predictions.xlsx"

GPU_CONFIG = {
    'memory_fraction_high_end': 0.6,
    'memory_fraction_low_end': 0.5,
    'memory_threshold_gb': 6.0,
    'batch_size_high_end': 1024,
    'batch_size_low_end': 512,
    'small_retry_batch': 64,
    'evaluation_batch': 256,
    'enable_cudnn_benchmark': True,
    'cudnn_deterministic': True,
    'enable_mixed_precision': True,
}

OPTIMIZATION_CONFIG = {
    'population_size': 2048,
    'max_generations': 2500,
    'elite_ratio': 0.15,
    'mutation_rate': 0.2,
    'crossover_rate': 0.8,
    'tournament_size': 8,
    'use_hybrid_optimization': True,
    'genetic_algorithm_ratio': 0.8,
    'adam_iterations': 1500,
    'adam_learning_rate': 0.02,
    'adam_weight_decay': 1e-5,
    'adam_lr_decay': 0.99,
    'tolerance': 1e-8,
    'patience': 50,
    'adam_patience': 30,
    'convergence_check_window': 50,
    'gradient_clip_norm': 1.0,
    'max_optimization_time_minutes': 90,
    'progress_report_interval': 50,
}

PHYSICS_CONFIG = {
    'gas_constant': 8.314,
    'reference_grain_size': 50e-6,
    'numerical_eps': 1e-8,
    'max_exp_value': 50.0,
    'min_positive_value': 1e-10,
    'strain_upper_limit': 10.0,
    'strain_c_upper_limit': 5.0,
    'strain_05_upper_limit': 5.0,
    'beta_d_range': (0.1, 100.0),
    'k_d_range': (0.5, 10.0),
    'exponent_range': (-10.0, 10.0),
    'constraint_penalty_mild': 100.0,
    'constraint_penalty_range': 50.0,
    'constraint_penalty_severe': 1000.0,
}

PARAMETER_BOUNDS = [
    (0.05, 0.95, 'a', 'Critical strain ratio coefficient'),
    (1e-10, 1e-7, 'a_1', 'Material constant for strain_p'),
    (-3.0, 3.0, 'n_1', 'Grain size exponent for strain_p'),
    (0.1, 2.0, 'm_1', 'Strain rate sensitivity for strain_p'),
    (50000, 800000, 'Q0_1', 'Base activation energy for strain_p (J/mol)'),
    (-50000, 50000, 'alpha_1', 'Temperature coefficient for strain_p'),
    (-100000, 100000, 'beta_1', 'Strain rate coefficient for strain_p'),
    (0.1, 1000.0, 'beta_d', 'Dynamic parameter'),
    (0.5, 15.0, 'k_d', 'Dynamic exponent'),
    (1e-10, 1e-7, 'a_2', 'Material constant for strain_05'),
    (-3.0, 3.0, 'n_2', 'Grain size exponent for strain_05'),
    (0.1, 2.0, 'm_2', 'Strain rate sensitivity for strain_05'),
    (50000, 800000, 'Q0_2', 'Base activation energy for strain_05 (J/mol)'),
    (-50000, 50000, 'alpha_2', 'Temperature coefficient for strain_05'),
    (-100000, 100000, 'beta_2', 'Strain rate coefficient for strain_05'),
]

BOUNDS = [(bound[0], bound[1]) for bound in PARAMETER_BOUNDS]
PARAM_NAMES = [bound[2] for bound in PARAMETER_BOUNDS]

PREDICTION_CONFIG = {
    'strain_column_index': 0,
    'temperature_column_index': 1,
    'rate_column_index': 2,
    'has_header': False,
    'sheet_parameter': "strain",
    'row_parameter': "temperature",
    'column_parameter': "rate",
}

FITTING_WEIGHTS = {
    'x_drx_weight': 1.0,
    'strain_05_weight': 0.5,
    'strain_c_weight': 0.5,
    'mse_weight': 1.0,
    'r2_weight': 0.5,
    'correlation_weight': 0.5,
}

FILE_CONFIG = {
    'excel_sheets': {
        'predictions': 'Fitting Results',
        'parameters': 'Best Parameters',
        'convergence': 'Optimization History',
        'configuration': 'Configuration',
        'performance': 'Performance Metrics',
        'processing_results': 'Processing Results',
    },
    'plot_dpi': 300,
    'plot_size': (15, 12),
    'font_size': 10,
}

DEBUG_CONFIG = {
    'enable_debug_output': False,
    'debug_first_forward_pass': False,
    'show_parameter_ranges': False,
    'show_fitness_details': False,
    'show_gpu_info': True,
    'show_optimization_progress': True,
    'show_convergence_details': True,
    'verbose_adam_output': False,
    'monitor_memory_usage': True,
    'monitor_computation_time': True,
}

INITIALIZATION_CONFIG = {
    'use_improved_initialization': True,
    'initialization_std_factors': {
        'a': 0.15,
        'log_a_std': 0.5,
        'n_std': 1.0,
        'm_std': 0.3,
        'Q0_std': 100000,
        'alpha_std': 2000,
        'beta_std': 10000,
        'log_beta_d_std': 1.0,
        'k_d_std': 1.0,
    },
    'initialization_centers': {
        'a_center': 0.5,
        'log_a_center': -8.5,
        'n_center': 0.0,
        'm_center': 1.0,
        'Q0_center': 300000,
        'alpha_center': 0,
        'beta_center': 0,
        'log_beta_d_center': 2.0,
        'k_d_center': 2.0,
    },
}

def process_excel_data():
    jisuan_file = os.path.join(BASE_DIR, INPUT_FILE)
    data_file = os.path.join(BASE_DIR, DATA_FILE)

    print("Reading data...")

    drx_df = pd.read_excel(jisuan_file, sheet_name="drx")
    print(f"drx data shape: {drx_df.shape}")

    stress_p_df = pd.read_excel(jisuan_file, sheet_name="stress_p", index_col=0)
    strain_p_df = pd.read_excel(jisuan_file, sheet_name="strain_p", index_col=0)
    stress_s_df = pd.read_excel(jisuan_file, sheet_name="stress_s", index_col=0)
    strain_c_df = pd.read_excel(jisuan_file, sheet_name="strain_c", index_col=0)

    print(f"stress_p data shape: {stress_p_df.shape}")

    print("\nReading data.xlsx...")

    data_sheets = {}
    rates = ["0.01", "0.1", "1", "10"]

    for rate in rates:
        sheet_data = pd.read_excel(data_file, sheet_name=rate)
        data_sheets[rate] = sheet_data
        print(f"Rate {rate} data shape: {sheet_data.shape}")

    temperatures = stress_p_df.columns.tolist()
    rates_numeric = stress_p_df.index.tolist()

    print(f"\nTemperature range: {temperatures}")
    print(f"Rate range: {rates_numeric}")

    print("\nCalculating stress_05 values...")

    stress_05_data = []

    for temp in temperatures:
        for rate in rates_numeric:
            try:
                stress_p_val = stress_p_df.loc[rate, temp]
                stress_s_val = stress_s_df.loc[rate, temp]

                stress_05_val = stress_p_val - 0.5 * (stress_p_val - stress_s_val)

                stress_05_data.append(
                    {"temperature": temp, "rate": rate, "stress_05": stress_05_val}
                )

            except KeyError as e:
                print(f"Warning: Missing data for T={temp}, rate={rate}: {e}")
                continue

    stress_05_df = pd.DataFrame(stress_05_data)
    print(f"stress_05 dataframe shape: {stress_05_df.shape}")

    print("\nFinding corresponding strain values...")

    strain_05_data = []

    for _, row in stress_05_df.iterrows():
        temp = row["temperature"]
        rate = row["rate"]
        target_stress = row["stress_05"]

        try:
            if isinstance(rate, (int, float)):
                if rate == int(rate):
                    rate_str = str(int(rate))
                else:
                    rate_str = str(rate)
            else:
                rate_str = str(rate)

            if rate_str not in data_sheets:
                print(f"Warning: Rate {rate} (converted to '{rate_str}') sheet not found")
                continue

            sheet_data = data_sheets[rate_str]

            temp_index = None
            temp_list = [850, 900, 950, 1000, 1050, 1100, 1150]
            if temp in temp_list:
                temp_index = temp_list.index(temp)
            else:
                print(f"Warning: Temperature {temp} not in expected range")
                continue

            strain_col_idx = temp_index * 2
            stress_col_idx = temp_index * 2 + 1

            if stress_col_idx >= len(sheet_data.columns):
                print(f"Warning: Column index out of range for T={temp}")
                continue

            strain_col = sheet_data.iloc[:, strain_col_idx]
            stress_col = sheet_data.iloc[:, stress_col_idx]

            valid_mask = ~(pd.isna(strain_col) | pd.isna(stress_col))
            strain_clean = strain_col[valid_mask].astype(float)
            stress_clean = stress_col[valid_mask].astype(float)

            strain_p_val = strain_p_df.loc[rate, temp]

            filter_mask = strain_clean >= strain_p_val
            strain_filtered = strain_clean[filter_mask]
            stress_filtered = stress_clean[filter_mask]

            if len(strain_filtered) == 0:
                print(f"Warning: No valid data after filtering for T={temp}, rate={rate}")
                continue

            if target_stress in stress_filtered.values:
                matching_indices = stress_filtered[stress_filtered == target_stress].index
                strain_05_val = strain_filtered.loc[matching_indices[0]]
            else:
                if len(stress_filtered) < 2:
                    print(f"Warning: Insufficient data points for interpolation at T={temp}, rate={rate}")
                    continue

                stress_sorted = stress_filtered.sort_values()
                strain_sorted = strain_filtered.loc[stress_sorted.index]

                if (target_stress < stress_sorted.min() or target_stress > stress_sorted.max()):
                    print(f"Warning: Target stress {target_stress} out of range for T={temp}, rate={rate}")
                    continue

                interp_func = interp1d(stress_sorted, strain_sorted, kind="linear")
                strain_05_val = float(interp_func(target_stress))

            strain_05_data.append(
                {
                    "temperature": temp,
                    "rate": rate,
                    "strain_05": strain_05_val,
                    "stress_05": target_stress,
                }
            )

        except Exception as e:
            print(f"Error processing T={temp}, rate={rate}: {e}")
            continue

    strain_05_df = pd.DataFrame(strain_05_data)
    print(f"strain_05 dataframe shape: {strain_05_df.shape}")

    print("\nSaving results...")

    output_file = os.path.join(BASE_DIR, OUTPUT_PROCESSING_RESULTS)

    with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
        strain_05_df.to_excel(writer, sheet_name="strain_05_complete", index=False)
        drx_df.to_excel(writer, sheet_name="drx_original", index=False)

    print(f"Results saved to: {output_file}")

    return drx_df, strain_05_df, strain_c_df, strain_p_df

def read_prediction_conditions():
    try:
        prediction_file = os.path.join(BASE_DIR, PREDICTION_CONDITIONS_FILE)

        if not os.path.exists(prediction_file):
            print(f"Warning: Prediction conditions file not found, skipping predictions")
            return None

        if PREDICTION_CONFIG['has_header']:
            prediction_df = pd.read_excel(prediction_file)
        else:
            prediction_df = pd.read_excel(prediction_file, header=None)

        required_columns = max(
            PREDICTION_CONFIG['strain_column_index'],
            PREDICTION_CONFIG['temperature_column_index'],
            PREDICTION_CONFIG['rate_column_index']
        ) + 1

        if prediction_df.shape[1] < required_columns:
            print(f"Error: Prediction file requires at least {required_columns} columns")
            return None

        strain_data = prediction_df.iloc[:, PREDICTION_CONFIG['strain_column_index']].dropna().unique()
        temperature_data = prediction_df.iloc[:, PREDICTION_CONFIG['temperature_column_index']].dropna().unique()
        rate_data = prediction_df.iloc[:, PREDICTION_CONFIG['rate_column_index']].dropna().unique()

        prediction_combinations = []
        for strain in strain_data:
            for temp in temperature_data:
                for rate in rate_data:
                    prediction_combinations.append(
                        {"strain": strain, "temperature": temp, "rate": rate}
                    )

        prediction_df_processed = pd.DataFrame(prediction_combinations)
        print(f"Generated {len(prediction_df_processed)} prediction combinations")

        return prediction_df_processed

    except Exception as e:
        print(f"Error reading prediction conditions: {e}")
        import traceback
        traceback.print_exc()
        return None

def organize_predictions_by_parameter(predictions_df, param_name):
    if param_name == "strain":
        index_param = "temperature"
        column_param = "rate"
        sheet_values = sorted(predictions_df["strain"].unique())
    elif param_name == "temperature":
        index_param = "strain"
        column_param = "rate"
        sheet_values = sorted(predictions_df["temperature"].unique())
    elif param_name == "rate":
        index_param = "strain"
        column_param = "temperature"
        sheet_values = sorted(predictions_df["rate"].unique())
    else:
        raise ValueError(f"Unknown parameter: {param_name}")

    organized_data = {}

    for sheet_val in sheet_values:
        subset = predictions_df[predictions_df[param_name] == sheet_val]

        if len(subset) == 0:
            continue

        pivot_table = subset.pivot_table(
            index=index_param, columns=column_param, values="X_drx_predicted", aggfunc="mean"
        )

        if param_name == "strain":
            sheet_name = str(sheet_val)
        else:
            sheet_name = f"{param_name}_{sheet_val}"

        organized_data[sheet_name] = pivot_table

    return organized_data

def validate_config():
    warnings = []

    if GPU_CONFIG['batch_size_high_end'] <= 0:
        warnings.append("GPU batch size must be positive")

    if OPTIMIZATION_CONFIG['population_size'] <= 0:
        warnings.append("Population size must be positive")

    if not 0 <= OPTIMIZATION_CONFIG['elite_ratio'] <= 1:
        warnings.append("Elite ratio must be between 0-1")

    if not 0 <= OPTIMIZATION_CONFIG['mutation_rate'] <= 1:
        warnings.append("Mutation rate must be between 0-1")

    if PHYSICS_CONFIG['numerical_eps'] <= 0:
        warnings.append("Numerical epsilon must be positive")

    for i, (low, high) in enumerate(BOUNDS):
        if low >= high:
            warnings.append(f"Invalid bounds for parameter {PARAM_NAMES[i]}: {low} >= {high}")

    if warnings:
        print("Configuration warnings:")
        for warning in warnings:
            print(f"  - {warning}")
        print()

    return len(warnings) == 0

def print_config_summary():
    print("Configuration Summary:")
    print(f"  Genetic Algorithm: Population {OPTIMIZATION_CONFIG['population_size']}, "
          f"Generations {OPTIMIZATION_CONFIG['max_generations']}")
    print(f"  Hybrid Optimization: {'Enabled' if OPTIMIZATION_CONFIG['use_hybrid_optimization'] else 'Disabled'}")
    print(f"  GPU Memory: {GPU_CONFIG['memory_fraction_high_end']*100:.0f}% (high-end)")
    print(f"  Batch Size: {GPU_CONFIG['batch_size_high_end']} (high-end)")
    print(f"  Base Directory: {BASE_DIR}")
    print(f"  Physical Model: Optimized activation energy Q = Q0 + alpha/T + beta*ln(strain_rate)")
    print()

@dataclass
class OptimizationConfig:
    def __init__(self):
        self.population_size = OPTIMIZATION_CONFIG['population_size']
        self.max_generations = OPTIMIZATION_CONFIG['max_generations']
        self.elite_ratio = OPTIMIZATION_CONFIG['elite_ratio']
        self.mutation_rate = OPTIMIZATION_CONFIG['mutation_rate']
        self.crossover_rate = OPTIMIZATION_CONFIG['crossover_rate']
        self.tournament_size = OPTIMIZATION_CONFIG['tournament_size']

        self.use_hybrid = OPTIMIZATION_CONFIG['use_hybrid_optimization']
        self.genetic_ratio = OPTIMIZATION_CONFIG['genetic_algorithm_ratio']
        self.adam_iterations = OPTIMIZATION_CONFIG['adam_iterations']

        self.tolerance = OPTIMIZATION_CONFIG['tolerance']
        self.patience = OPTIMIZATION_CONFIG['patience']

def check_gpu_status():
    print("Checking GPU status...")

    if not torch.cuda.is_available():
        print("CUDA is not available! Possible reasons:")
        print("   1. PyTorch not installed with CUDA support")
        print("   2. CUDA driver version mismatch")
        print("   3. No CUDA-compatible GPU")
        return False

    try:
        device_count = torch.cuda.device_count()
        print(f"Detected {device_count} CUDA device(s)")

        for i in range(device_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1e9
            print(f"   GPU {i}: {props.name} ({memory_gb:.1f} GB)")

        test_tensor = torch.randn(1000, 1000, device='cuda:0')
        print(f"GPU memory allocation test passed")
        del test_tensor
        torch.cuda.empty_cache()

        return True

    except Exception as e:
        print(f"GPU test failed: {e}")
        return False

def setup_gpu_environment():
    if DEBUG_CONFIG['show_gpu_info']:
        print("Initializing GPU environment...")

    if not torch.cuda.is_available():
        print("CUDA not available, please fix PyTorch CUDA installation")
        return None, GPU_CONFIG['batch_size_low_end']

    device = torch.device("cuda:0")
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9

    if DEBUG_CONFIG['show_gpu_info']:
        print(f"GPU: {gpu_name}")
        print(f"Memory: {gpu_memory:.1f} GB")

    torch.backends.cudnn.benchmark = GPU_CONFIG['enable_cudnn_benchmark']
    torch.backends.cudnn.deterministic = GPU_CONFIG['cudnn_deterministic']

    torch.cuda.empty_cache()
    if gpu_memory >= GPU_CONFIG['memory_threshold_gb']:
        torch.cuda.set_per_process_memory_fraction(GPU_CONFIG['memory_fraction_high_end'])
        batch_size = GPU_CONFIG['batch_size_high_end']
    else:
        torch.cuda.set_per_process_memory_fraction(GPU_CONFIG['memory_fraction_low_end'])
        batch_size = GPU_CONFIG['batch_size_low_end']

    if DEBUG_CONFIG['show_gpu_info']:
        print(f"Batch size: {batch_size}")
        print(f"Mixed precision: {'Enabled' if GPU_CONFIG['enable_mixed_precision'] else 'Disabled'}")

    return device, batch_size

class GPUPhysicsModel(nn.Module):
    def __init__(self, device):
        super().__init__()
        self.device = device

        self.register_buffer('R_GAS', torch.tensor(PHYSICS_CONFIG['gas_constant'], dtype=torch.float32, device=device))
        self.register_buffer('D_0', torch.tensor(PHYSICS_CONFIG['reference_grain_size'], dtype=torch.float32, device=device))

        self.register_buffer('EPS', torch.tensor(PHYSICS_CONFIG['numerical_eps'], dtype=torch.float32, device=device))
        self.register_buffer('MAX_EXP', torch.tensor(PHYSICS_CONFIG['max_exp_value'], dtype=torch.float32, device=device))

        bounds_tensor = torch.tensor(BOUNDS, dtype=torch.float32, device=device)
        self.register_buffer('bounds', bounds_tensor)

    def calculate_activation_energy_Q1(self, rates, temps_K, Q0_1, alpha_1, beta_1):
        device = rates.device
        EPS = PHYSICS_CONFIG['numerical_eps']

        rates_safe = torch.clamp(rates, min=EPS)
        temps_K_safe = torch.clamp(temps_K, min=EPS)

        temp_term = alpha_1 / temps_K_safe
        rate_term = beta_1 * torch.log(rates_safe)

        Q1 = Q0_1 + temp_term + rate_term

        Q1 = torch.clamp(Q1, min=1000, max=2000000)

        return Q1

    def calculate_activation_energy_Q2(self, rates, temps_K, Q0_2, alpha_2, beta_2):
        device = rates.device
        EPS = PHYSICS_CONFIG['numerical_eps']

        rates_safe = torch.clamp(rates, min=EPS)
        temps_K_safe = torch.clamp(temps_K, min=EPS)

        temp_term = alpha_2 / temps_K_safe
        rate_term = beta_2 * torch.log(rates_safe)

        Q2 = Q0_2 + temp_term + rate_term

        Q2 = torch.clamp(Q2, min=1000, max=2000000)

        return Q2

    def forward(self, params_batch, data_batch):
        batch_size = params_batch.shape[0]
        device = params_batch.device

        rates = data_batch['rates'].unsqueeze(0).expand(batch_size, -1)
        temps_K = data_batch['temps_K'].unsqueeze(0).expand(batch_size, -1)
        strains = data_batch['strains'].unsqueeze(0).expand(batch_size, -1)

        a = params_batch[:, 0:1]
        a_1 = params_batch[:, 1:2]
        n_1 = params_batch[:, 2:3]
        m_1 = params_batch[:, 3:4]
        Q0_1 = params_batch[:, 4:5]
        alpha_1 = params_batch[:, 5:6]
        beta_1 = params_batch[:, 6:7]
        beta_d = params_batch[:, 7:8]
        k_d = params_batch[:, 8:9]
        a_2 = params_batch[:, 9:10]
        n_2 = params_batch[:, 10:11]
        m_2 = params_batch[:, 11:12]
        Q0_2 = params_batch[:, 12:13]
        alpha_2 = params_batch[:, 13:14]
        beta_2 = params_batch[:, 14:15]

        EPS = PHYSICS_CONFIG['numerical_eps']
        MAX_EXP = PHYSICS_CONFIG['max_exp_value']
        R_GAS_VAL = PHYSICS_CONFIG['gas_constant']
        D_0_VAL = PHYSICS_CONFIG['reference_grain_size']

        def safe_exp_inline(x):
            x_clamped = torch.clamp(x, min=-MAX_EXP, max=MAX_EXP)
            return torch.exp(x_clamped)

        def safe_log_inline(x):
            x_clamped = torch.clamp(x, min=EPS)
            return torch.log(x_clamped)

        def safe_pow_inline(base, exponent):
            base_clamped = torch.clamp(base, min=EPS)
            exp_min, exp_max = PHYSICS_CONFIG['exponent_range']
            exponent_clamped = torch.clamp(exponent, min=exp_min, max=exp_max)
            return torch.pow(base_clamped, exponent_clamped)

        def safe_divide(numerator, denominator):
            denominator_safe = torch.clamp(torch.abs(denominator), min=EPS)
            return numerator / denominator_safe

        Q1 = self.calculate_activation_energy_Q1(rates, temps_K, Q0_1, alpha_1, beta_1)
        Q2 = self.calculate_activation_energy_Q2(rates, temps_K, Q0_2, alpha_2, beta_2)

        D0_term = torch.full_like(a_1, D_0_VAL)
        R_gas_term = torch.tensor(R_GAS_VAL, device=device, dtype=torch.float32)

        Q1_over_RT = torch.clamp(Q1 / (R_gas_term * temps_K), min=-MAX_EXP, max=MAX_EXP)
        Q2_over_RT = torch.clamp(Q2 / (R_gas_term * temps_K), min=-MAX_EXP, max=MAX_EXP)

        strain_p = a_1 * safe_pow_inline(D0_term, n_1) * safe_pow_inline(rates, m_1) * safe_exp_inline(Q1_over_RT)
        strain_c = a * strain_p
        strain_05 = a_2 * safe_pow_inline(D0_term, n_2) * safe_pow_inline(rates, m_2) * safe_exp_inline(Q2_over_RT)

        strain_p = torch.clamp(strain_p, min=EPS, max=PHYSICS_CONFIG['strain_upper_limit'])
        strain_c = torch.clamp(strain_c, min=EPS, max=PHYSICS_CONFIG['strain_c_upper_limit'])
        strain_05 = torch.clamp(strain_05, min=EPS, max=PHYSICS_CONFIG['strain_05_upper_limit'])

        strain_ratio = torch.clamp(safe_divide(strains - strain_c, strain_05), min=0.0, max=100.0)

        beta_d_min, beta_d_max = PHYSICS_CONFIG['beta_d_range']
        k_d_min, k_d_max = PHYSICS_CONFIG['k_d_range']
        beta_d_clamped = torch.clamp(beta_d, min=beta_d_min, max=beta_d_max)
        k_d_clamped = torch.clamp(k_d, min=k_d_min, max=k_d_max)

        power_term = safe_pow_inline(strain_ratio, k_d_clamped)
        exponent = torch.clamp(-beta_d_clamped * power_term, min=-MAX_EXP, max=0.0)

        x_drx = torch.clamp(1.0 - safe_exp_inline(exponent), min=0.0, max=1.0)

        return {
            'x_drx': x_drx,
            'strain_p': strain_p,
            'strain_c': strain_c,
            'strain_05': strain_05,
            'Q1': Q1,
            'Q2': Q2,
        }

class GPUGeneticOptimizer:
    def __init__(self, model, config, device):
        self.model = model
        self.config = config
        self.device = device

        self.population = self._init_population()
        self.fitness_history = []
        self.best_individual = None
        self.best_fitness = float('inf')

    def _init_population(self):
        pop_size = self.config.population_size
        n_params = len(BOUNDS)

        population = torch.zeros(pop_size, n_params, device=self.device, dtype=torch.float32)

        if INITIALIZATION_CONFIG['use_improved_initialization']:
            std_factors = INITIALIZATION_CONFIG['initialization_std_factors']
            centers = INITIALIZATION_CONFIG['initialization_centers']

            population[:, 0] = torch.normal(centers['a_center'], std_factors['a'],
                                          (pop_size,), device=self.device).clamp(0.05, 0.95)

            log_a1 = torch.normal(centers['log_a_center'], std_factors['log_a_std'],
                                (pop_size,), device=self.device)
            population[:, 1] = torch.exp(log_a1).clamp(1e-10, 1e-7)
            log_a2 = torch.normal(centers['log_a_center'], std_factors['log_a_std'],
                                (pop_size,), device=self.device)
            population[:, 9] = torch.exp(log_a2).clamp(1e-10, 1e-7)

            population[:, 2] = torch.normal(centers['n_center'], std_factors['n_std'],
                                          (pop_size,), device=self.device).clamp(-3.0, 3.0)
            population[:, 10] = torch.normal(centers['n_center'], std_factors['n_std'],
                                           (pop_size,), device=self.device).clamp(-3.0, 3.0)

            population[:, 3] = torch.normal(centers['m_center'], std_factors['m_std'],
                                          (pop_size,), device=self.device).clamp(0.1, 2.0)
            population[:, 11] = torch.normal(centers['m_center'], std_factors['m_std'],
                                           (pop_size,), device=self.device).clamp(0.1, 2.0)

            population[:, 4] = torch.normal(centers['Q0_center'], std_factors['Q0_std'],
                                          (pop_size,), device=self.device).clamp(50000, 800000)
            population[:, 12] = torch.normal(centers['Q0_center'], std_factors['Q0_std'],
                                           (pop_size,), device=self.device).clamp(50000, 800000)

            population[:, 5] = torch.normal(centers['alpha_center'], std_factors['alpha_std'],
                                          (pop_size,), device=self.device).clamp(-50000, 50000)
            population[:, 13] = torch.normal(centers['alpha_center'], std_factors['alpha_std'],
                                           (pop_size,), device=self.device).clamp(-50000, 50000)

            population[:, 6] = torch.normal(centers['beta_center'], std_factors['beta_std'],
                                          (pop_size,), device=self.device).clamp(-100000, 100000)
            population[:, 14] = torch.normal(centers['beta_center'], std_factors['beta_std'],
                                           (pop_size,), device=self.device).clamp(-100000, 100000)

            log_beta_d = torch.normal(centers['log_beta_d_center'], std_factors['log_beta_d_std'],
                                    (pop_size,), device=self.device)
            population[:, 7] = torch.exp(log_beta_d).clamp(0.1, 1000.0)

            population[:, 8] = torch.normal(centers['k_d_center'], std_factors['k_d_std'],
                                          (pop_size,), device=self.device).clamp(0.5, 15.0)
        else:
            for i, (low, high) in enumerate(BOUNDS):
                population[:, i] = torch.uniform(low, high, (pop_size,), device=self.device)

        return population

    def evaluate_fitness(self, data_batch, target_batch):
        batch_size = min(GPU_CONFIG['evaluation_batch'], self.config.population_size)
        all_fitness = []

        for i in range(0, self.config.population_size, batch_size):
            end_idx = min(i + batch_size, self.config.population_size)
            pop_batch = self.population[i:end_idx]

            try:
                if GPU_CONFIG['enable_mixed_precision']:
                    with torch.cuda.amp.autocast():
                        predictions = self.model(pop_batch, data_batch)
                        fitness_batch = self._compute_fitness(predictions, target_batch)
                else:
                    predictions = self.model(pop_batch, data_batch)
                    fitness_batch = self._compute_fitness(predictions, target_batch)

                all_fitness.append(fitness_batch)

            except RuntimeError as e:
                if "out of memory" in str(e):
                    torch.cuda.empty_cache()
                    small_batch = GPU_CONFIG['small_retry_batch']
                    for j in range(i, end_idx, small_batch):
                        end_small = min(j + small_batch, end_idx)
                        pop_small = self.population[j:end_small]
                        predictions = self.model(pop_small, data_batch)
                        fitness_small = self._compute_fitness(predictions, target_batch)
                        all_fitness.append(fitness_small)
                    continue
                else:
                    raise e

        return torch.cat(all_fitness, dim=0)

    def _compute_fitness(self, predictions, targets):
        x_drx_pred = predictions['x_drx']
        x_drx_target = targets['x_drx'].unsqueeze(0).expand_as(x_drx_pred)

        strain_05_pred = predictions['strain_05']
        strain_05_target = targets['strain_05'].unsqueeze(0).expand_as(strain_05_pred)

        strain_c_pred = predictions['strain_c']
        strain_c_target = targets['strain_c'].unsqueeze(0).expand_as(strain_c_pred)

        mse_x_drx = torch.mean((x_drx_pred - x_drx_target) ** 2, dim=1)
        mse_strain_05 = torch.mean((strain_05_pred - strain_05_target) ** 2, dim=1)
        mse_strain_c = torch.mean((strain_c_pred - strain_c_target) ** 2, dim=1)

        weighted_mse = (FITTING_WEIGHTS['x_drx_weight'] * mse_x_drx +
                       FITTING_WEIGHTS['strain_05_weight'] * mse_strain_05 +
                       FITTING_WEIGHTS['strain_c_weight'] * mse_strain_c)

        batch_size = x_drx_pred.shape[0]
        penalty = torch.zeros(batch_size, device=x_drx_pred.device, dtype=x_drx_pred.dtype)

        strain_c_penalty = torch.where(predictions['strain_c'] <= 0, 1.0, 0.0)
        strain_05_penalty = torch.where(predictions['strain_05'] <= 0, 1.0, 0.0)

        penalty += torch.sum(strain_c_penalty, dim=1) * PHYSICS_CONFIG['constraint_penalty_mild']
        penalty += torch.sum(strain_05_penalty, dim=1) * PHYSICS_CONFIG['constraint_penalty_mild']

        x_drx_range_penalty = torch.where((x_drx_pred < 0) | (x_drx_pred > 1), 1.0, 0.0)
        penalty += torch.sum(x_drx_range_penalty, dim=1) * PHYSICS_CONFIG['constraint_penalty_range']

        nan_inf_penalty = torch.where(torch.isnan(weighted_mse) | torch.isinf(weighted_mse),
                                    PHYSICS_CONFIG['constraint_penalty_severe'], 0.0)
        penalty += nan_inf_penalty

        fitness = weighted_mse + penalty

        return fitness

    def evolve_generation(self, data_batch, target_batch):
        start_time = time.time()

        fitness = self.evaluate_fitness(data_batch, target_batch)

        best_idx = torch.argmin(fitness)
        current_best = fitness[best_idx].item()

        if current_best < self.best_fitness:
            self.best_fitness = current_best
            self.best_individual = self.population[best_idx].clone()

        self._evolve_population(fitness)

        self.fitness_history.append(self.best_fitness)

        return self.best_fitness, time.time() - start_time

    def _evolve_population(self, fitness):
        pop_size = self.config.population_size
        elite_size = int(pop_size * self.config.elite_ratio)

        elite_indices = torch.topk(fitness, elite_size, largest=False)[1]
        new_population = torch.zeros_like(self.population)
        new_population[:elite_size] = self.population[elite_indices]

        for i in range(elite_size, pop_size):
            parent1_idx = self._tournament_selection(fitness)
            parent2_idx = self._tournament_selection(fitness)

            parent1 = self.population[parent1_idx]
            parent2 = self.population[parent2_idx]

            if torch.rand(1, device=self.device) < self.config.crossover_rate:
                alpha = torch.rand(len(BOUNDS), device=self.device)
                child = alpha * parent1 + (1 - alpha) * parent2
            else:
                child = parent1.clone()

            if torch.rand(1, device=self.device) < self.config.mutation_rate:
                mutation_mask = torch.rand(len(BOUNDS), device=self.device) < 0.3
                mutation = torch.randn(len(BOUNDS), device=self.device) * 0.1
                child[mutation_mask] += mutation[mutation_mask]

            for j, (low, high) in enumerate(BOUNDS):
                child[j] = torch.clamp(child[j], low, high)

            new_population[i] = child

        self.population = new_population

    def _tournament_selection(self, fitness):
        tournament_indices = torch.randint(
            0, len(fitness),
            (self.config.tournament_size,),
            device=self.device
        )
        tournament_fitness = fitness[tournament_indices]
        winner_idx = torch.argmin(tournament_fitness)
        return tournament_indices[winner_idx]

class AdamFinetuner:
    def __init__(self, model, config, device):
        self.model = model
        self.config = config
        self.device = device

    def finetune(self, initial_params, data_batch, target_batch):
        params = nn.Parameter(initial_params.clone().requires_grad_(True))
        optimizer = optim.AdamW([params],
                              lr=OPTIMIZATION_CONFIG['adam_learning_rate'],
                              weight_decay=OPTIMIZATION_CONFIG['adam_weight_decay'])
        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=OPTIMIZATION_CONFIG['adam_lr_decay'])

        best_loss = float('inf')
        best_params = params.clone()
        patience = 0
        max_patience = OPTIMIZATION_CONFIG['adam_patience']

        for iteration in range(self.config.adam_iterations):
            optimizer.zero_grad()

            with torch.no_grad():
                for i, (low, high) in enumerate(BOUNDS):
                    params.data[i] = torch.clamp(params.data[i], low, high)

            if GPU_CONFIG['enable_mixed_precision']:
                with torch.cuda.amp.autocast():
                    predictions = self.model(params.unsqueeze(0), data_batch)
                    loss = self._compute_loss(predictions, target_batch)
            else:
                predictions = self.model(params.unsqueeze(0), data_batch)
                loss = self._compute_loss(predictions, target_batch)

            loss.backward()
            torch.nn.utils.clip_grad_norm_([params], OPTIMIZATION_CONFIG['gradient_clip_norm'])
            optimizer.step()
            scheduler.step()

            current_loss = loss.item()
            if current_loss < best_loss:
                best_loss = current_loss
                best_params = params.clone()
                patience = 0
            else:
                patience += 1

            if patience >= max_patience:
                break

            if DEBUG_CONFIG['verbose_adam_output'] and iteration % OPTIMIZATION_CONFIG['progress_report_interval'] == 0:
                print(f"    Iteration {iteration}: Loss={current_loss:.6e}")

        return best_params.detach(), best_loss

    def _compute_loss(self, predictions, targets):
        x_drx_pred = predictions['x_drx'].squeeze()
        x_drx_target = targets['x_drx']

        strain_05_pred = predictions['strain_05'].squeeze()
        strain_05_target = targets['strain_05']

        strain_c_pred = predictions['strain_c'].squeeze()
        strain_c_target = targets['strain_c']

        mse_x_drx = torch.mean((x_drx_pred - x_drx_target) ** 2)
        mse_strain_05 = torch.mean((strain_05_pred - strain_05_target) ** 2)
        mse_strain_c = torch.mean((strain_c_pred - strain_c_target) ** 2)

        total_loss = (FITTING_WEIGHTS['x_drx_weight'] * mse_x_drx +
                     FITTING_WEIGHTS['strain_05_weight'] * mse_strain_05 +
                     FITTING_WEIGHTS['strain_c_weight'] * mse_strain_c)

        return total_loss

class FastGPUOptimizer:
    def __init__(self, config=None):
        self.config = config or OptimizationConfig()

        self.device, self.batch_size = setup_gpu_environment()
        if self.device is None:
            raise RuntimeError("GPU not available")

        self.model = GPUPhysicsModel(self.device)
        self.model = self.model.to(self.device)

        self.best_params = None
        self.best_fitness = float('inf')
        self.history = []

    def prepare_data(self, drx_df, strain_05_df, strain_c_df, strain_p_df):
        print("Preparing GPU data...")

        print(f"   Input data check:")
        print(f"     drx_df: {drx_df.shape if drx_df is not None else 'None'}")
        print(f"     strain_05_df: {strain_05_df.shape if strain_05_df is not None else 'None'}")
        print(f"     strain_c_df: {strain_c_df.shape if strain_c_df is not None else 'None'}")
        print(f"     strain_p_df: {strain_p_df.shape if strain_p_df is not None else 'None'}")

        if drx_df is None or len(drx_df) == 0:
            raise ValueError("drx_df is empty or None")

        required_drx_cols = ["temperature", "rate", "strain", "drx"]
        missing_cols = [col for col in required_drx_cols if col not in drx_df.columns]
        if missing_cols:
            raise ValueError(f"drx_df missing required columns: {missing_cols}")

        fitting_data = []
        processing_errors = []

        for idx, drx_row in drx_df.iterrows():
            try:
                temp = drx_row["temperature"]
                rate = drx_row["rate"]
                strain = drx_row["strain"]
                x_drx_measured = drx_row["drx"] / 100

                strain_05_match = strain_05_df[
                    (strain_05_df["temperature"] == temp) & (strain_05_df["rate"] == rate)
                ]

                if len(strain_05_match) == 0:
                    processing_errors.append(f"Row {idx}: No matching strain_05 data (T={temp}, rate={rate})")
                    continue

                strain_05_measured = strain_05_match.iloc[0]["strain_05"]

                try:
                    strain_c_measured = strain_c_df.loc[rate, temp]
                    strain_p_measured = strain_p_df.loc[rate, temp]
                except (KeyError, IndexError) as e:
                    processing_errors.append(f"Row {idx}: Missing strain_c or strain_p data (T={temp}, rate={rate}): {e}")
                    continue

                if (
                    strain <= strain_c_measured
                    or x_drx_measured <= 0
                    or x_drx_measured >= 1
                    or strain_05_measured <= 0
                    or strain_c_measured <= 0
                    or strain_p_measured <= 0
                    or strain_c_measured > strain_p_measured
                    or any(np.isnan([strain, temp, rate, x_drx_measured, strain_05_measured, strain_c_measured, strain_p_measured]))
                ):
                    processing_errors.append(f"Row {idx}: Data validation failed (T={temp}, rate={rate})")
                    continue

                fitting_data.append(
                    {
                        "temperature": temp,
                        "rate": rate,
                        "strain": strain,
                        "x_drx_measured": x_drx_measured,
                        "strain_05_measured": strain_05_measured,
                        "strain_c_measured": strain_c_measured,
                        "strain_p_measured": strain_p_measured,
                        "temp_K": temp + 273.15,
                    }
                )

            except Exception as e:
                processing_errors.append(f"Row {idx}: Processing error {e}")
                continue

        if processing_errors:
            print(f"   Data processing warnings ({len(processing_errors)}):")
            for error in processing_errors[:5]:
                print(f"     {error}")
            if len(processing_errors) > 5:
                print(f"     ... and {len(processing_errors) - 5} more")

        self.fitting_data = pd.DataFrame(fitting_data)

        if len(self.fitting_data) == 0:
            raise ValueError("No valid fitting data, check data quality and matching conditions")

        print(f"   Valid data points: {len(self.fitting_data)} / {len(drx_df)}")

        try:
            rates = torch.tensor(self.fitting_data['rate'].values, device=self.device, dtype=torch.float32)
            temps_K = torch.tensor(self.fitting_data['temp_K'].values, device=self.device, dtype=torch.float32)
            strains = torch.tensor(self.fitting_data['strain'].values, device=self.device, dtype=torch.float32)
            x_drx = torch.tensor(self.fitting_data['x_drx_measured'].values, device=self.device, dtype=torch.float32)
            strain_05 = torch.tensor(self.fitting_data['strain_05_measured'].values, device=self.device, dtype=torch.float32)
            strain_c = torch.tensor(self.fitting_data['strain_c_measured'].values, device=self.device, dtype=torch.float32)

            self.data_batch = {
                'rates': rates,
                'temps_K': temps_K,
                'strains': strains
            }

            self.target_batch = {
                'x_drx': x_drx,
                'strain_05': strain_05,
                'strain_c': strain_c,
            }

            print(f"Data preparation complete: {len(rates)} data points")

            return len(rates)

        except Exception as e:
            print(f"Tensor conversion failed: {e}")
            raise

    def optimize(self):
        if DEBUG_CONFIG['show_optimization_progress']:
            print(f"\nStarting GPU optimization...")
        start_time = time.time()

        try:
            genetic_opt = GPUGeneticOptimizer(self.model, self.config, self.device)

            n_generations = int(self.config.max_generations * self.config.genetic_ratio)
            if DEBUG_CONFIG['show_optimization_progress']:
                print(f"Genetic algorithm optimization ({n_generations} generations)...")

            for gen in range(n_generations):
                best_fitness, gen_time = genetic_opt.evolve_generation(
                    self.data_batch, self.target_batch
                )

                self.history.append({
                    'generation': gen,
                    'fitness': best_fitness,
                    'time': gen_time
                })

                if DEBUG_CONFIG['show_optimization_progress'] and gen % OPTIMIZATION_CONFIG['progress_report_interval'] == 0:
                    print(f"  Generation {gen}: Fitness={best_fitness:.6e}, Time={gen_time:.3f}s")

                if len(genetic_opt.fitness_history) > OPTIMIZATION_CONFIG['convergence_check_window']:
                    recent_change = abs(genetic_opt.fitness_history[-OPTIMIZATION_CONFIG['convergence_check_window']] -
                                      genetic_opt.fitness_history[-1])
                    if recent_change < self.config.tolerance:
                        if DEBUG_CONFIG['show_convergence_details']:
                            print(f"  Early convergence at generation {gen}")
                        break

                if time.time() - start_time > OPTIMIZATION_CONFIG['max_optimization_time_minutes'] * 60:
                    if DEBUG_CONFIG['show_optimization_progress']:
                        print(f"  Time limit reached at generation {gen}")
                    break

            if (self.config.use_hybrid and genetic_opt.best_individual is not None and
                genetic_opt.best_fitness != float('inf')):
                if DEBUG_CONFIG['show_optimization_progress']:
                    print(f"Adam fine-tuning...")
                adam_opt = AdamFinetuner(self.model, self.config, self.device)
                best_params_final, best_loss_final = adam_opt.finetune(
                    genetic_opt.best_individual, self.data_batch, self.target_batch
                )

                if best_loss_final < genetic_opt.best_fitness:
                    self.best_params = best_params_final.cpu().numpy()
                    self.best_fitness = best_loss_final
                else:
                    self.best_params = genetic_opt.best_individual.cpu().numpy()
                    self.best_fitness = genetic_opt.best_fitness
            else:
                if genetic_opt.best_individual is not None:
                    self.best_params = genetic_opt.best_individual.cpu().numpy()
                    self.best_fitness = genetic_opt.best_fitness
                else:
                    print("Warning: No valid solution found, using random parameters")
                    self.best_params = np.random.uniform([b[0] for b in BOUNDS], [b[1] for b in BOUNDS])
                    self.best_fitness = float('inf')

            total_time = time.time() - start_time

            if DEBUG_CONFIG['show_optimization_progress']:
                print(f"\nGPU optimization complete!")
                print(f"Total time: {total_time:.2f} seconds")
                print(f"Best fitness: {self.best_fitness:.6e}")

            return True

        except Exception as e:
            print(f"Optimization failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def get_results(self):
        if self.best_params is None:
            return None

        try:
            with torch.no_grad():
                params_tensor = torch.tensor(self.best_params, device=self.device, dtype=torch.float32).unsqueeze(0)
                predictions = self.model(params_tensor, self.data_batch)

                return {
                    'best_params': dict(zip(PARAM_NAMES, self.best_params)),
                    'best_fitness': self.best_fitness,
                    'predictions': {
                        'x_drx': predictions['x_drx'].squeeze().cpu().numpy(),
                        'strain_c': predictions['strain_c'].squeeze().cpu().numpy(),
                        'strain_05': predictions['strain_05'].squeeze().cpu().numpy(),
                        'strain_p': predictions['strain_p'].squeeze().cpu().numpy(),
                        'Q1': predictions['Q1'].squeeze().cpu().numpy(),
                        'Q2': predictions['Q2'].squeeze().cpu().numpy(),
                    },
                    'targets': {
                        'x_drx': self.target_batch['x_drx'].cpu().numpy(),
                        'strain_05': self.target_batch['strain_05'].cpu().numpy(),
                        'strain_c': self.target_batch['strain_c'].cpu().numpy(),
                    },
                    'fitting_data': self.fitting_data,
                    'history': self.history
                }
        except Exception as e:
            print(f"Failed to get results: {e}")
            return {
                'best_params': dict(zip(PARAM_NAMES, self.best_params)),
                'best_fitness': self.best_fitness,
                'history': self.history
            }

    def predict_conditions(self, prediction_df):
        if self.best_params is None:
            raise ValueError("No parameters available for prediction")

        predictions = []

        for _, row in prediction_df.iterrows():
            temp = row["temperature"]
            rate = row["rate"]
            strain = row["strain"]
            temp_K = temp + 273.15

            single_data = {
                'rates': torch.tensor([rate], device=self.device, dtype=torch.float32),
                'temps_K': torch.tensor([temp_K], device=self.device, dtype=torch.float32),
                'strains': torch.tensor([strain], device=self.device, dtype=torch.float32)
            }

            with torch.no_grad():
                params_tensor = torch.tensor(self.best_params, device=self.device, dtype=torch.float32).unsqueeze(0)
                pred = self.model(params_tensor, single_data)

                predictions.append({
                    "strain": strain,
                    "temperature": temp,
                    "rate": rate,
                    "X_drx_predicted": pred['x_drx'].cpu().item(),
                    "strain_p_predicted": pred['strain_p'].cpu().item(),
                    "strain_c_predicted": pred['strain_c'].cpu().item(),
                    "strain_05_predicted": pred['strain_05'].cpu().item(),
                    "Q1_optimized": pred['Q1'].cpu().item(),
                    "Q2_optimized": pred['Q2'].cpu().item(),
                })

        return pd.DataFrame(predictions)

def calculate_performance_metrics(predictions_df):
    metrics = {}

    x_drx_actual = predictions_df["x_drx_measured"].values
    x_drx_pred = predictions_df["x_drx_predicted"].values

    metrics["X_drx"] = {
        "MSE": np.mean((x_drx_actual - x_drx_pred) ** 2),
        "RMSE": np.sqrt(np.mean((x_drx_actual - x_drx_pred) ** 2)),
        "MAE": np.mean(np.abs(x_drx_actual - x_drx_pred)),
        "R2": 1 - np.sum((x_drx_actual - x_drx_pred) ** 2) / np.sum((x_drx_actual - np.mean(x_drx_actual)) ** 2),
        "Correlation": np.corrcoef(x_drx_actual, x_drx_pred)[0, 1],
    }

    strain_05_actual = predictions_df["strain_05_measured"].values
    strain_05_pred = predictions_df["strain_05_predicted"].values

    metrics["strain_05"] = {
        "MSE": np.mean((strain_05_actual - strain_05_pred) ** 2),
        "RMSE": np.sqrt(np.mean((strain_05_actual - strain_05_pred) ** 2)),
        "MAE": np.mean(np.abs(strain_05_actual - strain_05_pred)),
        "R2": 1 - np.sum((strain_05_actual - strain_05_pred) ** 2) / np.sum((strain_05_actual - np.mean(strain_05_actual)) ** 2),
        "Correlation": np.corrcoef(strain_05_actual, strain_05_pred)[0, 1],
    }

    strain_c_actual = predictions_df["strain_c_measured"].values
    strain_c_pred = predictions_df["strain_c_predicted"].values

    metrics["strain_c"] = {
        "MSE": np.mean((strain_c_actual - strain_c_pred) ** 2),
        "RMSE": np.sqrt(np.mean((strain_c_actual - strain_c_pred) ** 2)),
        "MAE": np.mean(np.abs(strain_c_actual - strain_c_pred)),
        "R2": 1 - np.sum((strain_c_actual - strain_c_pred) ** 2) / np.sum((strain_c_actual - np.mean(strain_c_actual)) ** 2),
        "Correlation": np.corrcoef(strain_c_actual, strain_c_pred)[0, 1],
    }

    return metrics

def create_fitting_plots(predictions_df, output_path):
    fig, axes = plt.subplots(2, 2, figsize=FILE_CONFIG['plot_size'])
    fig.suptitle("GPU-Accelerated Dynamic Recrystallization Model Fitting Results", fontsize=16, fontweight="bold")

    ax1 = axes[0, 0]
    x_drx_actual = predictions_df["x_drx_measured"]
    x_drx_pred = predictions_df["x_drx_predicted"]
    ax1.scatter(x_drx_actual, x_drx_pred, alpha=0.7, s=50)
    ax1.plot([0, 1], [0, 1], "r--", linewidth=2, label="Ideal fit")
    ax1.set_xlabel("X_drx Experimental")
    ax1.set_ylabel("X_drx Predicted")
    ax1.set_title("X_drx Fitting Performance")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    ax2 = axes[0, 1]
    strain_05_actual = predictions_df["strain_05_measured"]
    strain_05_pred = predictions_df["strain_05_predicted"]
    ax2.scatter(strain_05_actual, strain_05_pred, alpha=0.7, s=50, color="green")
    min_val = min(strain_05_actual.min(), strain_05_pred.min())
    max_val = max(strain_05_actual.max(), strain_05_pred.max())
    ax2.plot([min_val, max_val], [min_val, max_val], "r--", linewidth=2, label="Ideal fit")
    ax2.set_xlabel("_0.5 Experimental")
    ax2.set_ylabel("_0.5 Predicted")
    ax2.set_title("_0.5 Fitting Performance")
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    ax3 = axes[1, 0]
    strain_c_actual = predictions_df["strain_c_measured"]
    strain_c_pred = predictions_df["strain_c_predicted"]
    ax3.scatter(strain_c_actual, strain_c_pred, alpha=0.7, s=50, color="orange")
    min_val = min(strain_c_actual.min(), strain_c_pred.min())
    max_val = max(strain_c_actual.max(), strain_c_pred.max())
    ax3.plot([min_val, max_val], [min_val, max_val], "r--", linewidth=2, label="Ideal fit")
    ax3.set_xlabel("_c Experimental")
    ax3.set_ylabel("_c Predicted")
    ax3.set_title("_c Fitting Performance")
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    ax4 = axes[1, 1]
    Q1_values = predictions_df["Q1_optimized"]
    Q2_values = predictions_df["Q2_optimized"]
    ax4.hist(Q1_values, bins=20, alpha=0.6, label="Q1 Optimized", color="blue")
    ax4.hist(Q2_values, bins=20, alpha=0.6, label="Q2 Optimized", color="red")
    ax4.set_xlabel("Activation Energy (J/mol)")
    ax4.set_ylabel("Frequency")
    ax4.set_title("Optimized Activation Energy Distribution")
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=FILE_CONFIG['plot_dpi'], bbox_inches="tight")
    plt.show()

def create_results_plot(results, output_path=None):
    if output_path is None:
        output_path = os.path.join(BASE_DIR, OUTPUT_GA_PLOT)

    print("Creating result plots...")

    plot_size = FILE_CONFIG['plot_size']
    fig, axes = plt.subplots(2, 2, figsize=plot_size)
    fig.suptitle('GPU-Accelerated Dynamic Recrystallization Optimization Results', fontsize=16, fontweight='bold')

    ax1 = axes[0, 0]
    if 'predictions' in results:
        actual = results['targets']['x_drx']
        predicted = results['predictions']['x_drx']

        ax1.scatter(actual, predicted, alpha=0.7, s=50, c='blue')
        min_val, max_val = min(actual.min(), predicted.min()), max(actual.max(), predicted.max())
        ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)

        r2 = 1 - np.sum((actual - predicted)**2) / np.sum((actual - np.mean(actual))**2)
        ax1.text(0.05, 0.95, f'R = {r2:.4f}', transform=ax1.transAxes,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    ax1.set_xlabel('Actual X_drx')
    ax1.set_ylabel('Predicted X_drx')
    ax1.set_title('Prediction Accuracy')
    ax1.grid(True, alpha=0.3)

    ax2 = axes[0, 1]
    if results['history']:
        generations = [h['generation'] for h in results['history']]
        fitness = [h['fitness'] for h in results['history']]
        ax2.semilogy(generations, fitness, 'b-', linewidth=2)

    ax2.set_xlabel('Generation')
    ax2.set_ylabel('Fitness (log)')
    ax2.set_title('Convergence History')
    ax2.grid(True, alpha=0.3)

    ax3 = axes[1, 0]
    params = results['best_params']
    names = list(params.keys())
    values = list(params.values())

    y_pos = np.arange(len(names))
    ax3.barh(y_pos, values, alpha=0.7)
    ax3.set_yticks(y_pos)
    ax3.set_yticklabels(names)
    ax3.set_xlabel('Parameter Value')
    ax3.set_title('Best Parameters')
    ax3.grid(True, alpha=0.3)

    ax4 = axes[1, 1]
    performance = {
        'Best Fitness': f"{results['best_fitness']:.2e}",
        'Parameters': f"{len(values)}",
        'Generations': f"{len(results['history'])}"
    }

    if 'predictions' in results:
        actual = results['targets']['x_drx']
        predicted = results['predictions']['x_drx']
        r2 = 1 - np.sum((actual - predicted)**2) / np.sum((actual - np.mean(actual))**2)
        rmse = np.sqrt(np.mean((actual - predicted)**2))
        performance.update({
            'R Score': f"{r2:.4f}",
            'RMSE': f"{rmse:.4f}",
            'Data Points': f"{len(actual)}"
        })

    ax4.axis('off')
    table_data = [[k, v] for k, v in performance.items()]
    table = ax4.table(cellText=table_data,
                     colLabels=['Metric', 'Value'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2)
    ax4.set_title('Performance Statistics', pad=20)

    plt.tight_layout()
    plt.savefig(output_path, dpi=FILE_CONFIG['plot_dpi'], bbox_inches='tight')
    plt.show()

    print(f"Plots saved to: {output_path}")

def main():
    try:
        print("=" * 80)
        print("GPU-Accelerated Dynamic Recrystallization Analysis System")
        print("Optimized activation energy formula: Q = Q0 + alpha/T + beta*ln(strain_rate)")
        print("=" * 80)

        validate_config()
        print_config_summary()

        print("\nStep 1: Processing Excel data...")
        drx_df, strain_05_df, strain_c_df, strain_p_df = process_excel_data()

        print("\nStep 2: Initializing GPU optimization model...")
        config = OptimizationConfig()

        if DEBUG_CONFIG['show_optimization_progress']:
            print(f"Optimization configuration:")
            print(f"  Population size: {config.population_size}")
            print(f"  Max generations: {config.max_generations}")
            print(f"  Hybrid optimization: {config.use_hybrid}")
            print(f"  Expected time: < {OPTIMIZATION_CONFIG['max_optimization_time_minutes']} minutes")

        print(f"\nInitializing optimizer...")

        try:
            optimizer = FastGPUOptimizer(config)
        except Exception as e:
            print(f"Optimizer initialization failed: {e}")
            import traceback
            traceback.print_exc()
            raise

        try:
            print(f"\nPreparing training data...")
            n_points = optimizer.prepare_data(drx_df, strain_05_df, strain_c_df, strain_p_df)
            print(f"Data preparation successful: {n_points} valid data points")
        except Exception as e:
            print(f"Data preparation failed: {e}")
            import traceback
            traceback.print_exc()
            raise

        print(f"\nPre-testing model...")
        try:
            test_population = torch.zeros(10, len(BOUNDS), device=optimizer.device, dtype=torch.float32)
            for i, (low, high) in enumerate(BOUNDS):
                mid_val = (low + high) / 2
                test_population[:, i] = mid_val
            test_population += torch.randn_like(test_population) * 0.1
            for i, (low, high) in enumerate(BOUNDS):
                test_population[:, i] = torch.clamp(test_population[:, i], low, high)

            with torch.no_grad():
                test_predictions = optimizer.model(test_population, optimizer.data_batch)
                temp_genetic = GPUGeneticOptimizer(optimizer.model, config, optimizer.device)
                test_fitness = temp_genetic._compute_fitness(test_predictions, optimizer.target_batch)
                valid_count = torch.sum(torch.isfinite(test_fitness)).item()

                print(f"  Test complete: {valid_count}/10 parameter sets produced finite fitness")
                print(f"  Fitness range: [{torch.min(test_fitness):.2e}, {torch.max(test_fitness):.2e}]")

        except Exception as e:
            print(f"  Pre-test failed: {e}")
            print(f"  Continuing with full optimization...")

        print("\nStep 3: Executing GPU-accelerated model fitting...")
        start_time = time.time()
        success = optimizer.optimize()
        total_time = time.time() - start_time

        if not success:
            raise Exception("Model fitting failed")

        print("\nStep 4: Computing model predictions...")
        results = optimizer.get_results()

        if 'predictions' in results:
            predictions_df = pd.DataFrame({
                'temperature': results['fitting_data']['temperature'],
                'rate': results['fitting_data']['rate'],
                'strain': results['fitting_data']['strain'],
                'x_drx_measured': results['targets']['x_drx'],
                'x_drx_predicted': results['predictions']['x_drx'],
                'strain_05_measured': results['targets']['strain_05'],
                'strain_05_predicted': results['predictions']['strain_05'],
                'strain_c_measured': results['targets']['strain_c'],
                'strain_c_predicted': results['predictions']['strain_c'],
                'strain_p_predicted': results['predictions']['strain_p'],
                'Q1_optimized': results['predictions']['Q1'],
                'Q2_optimized': results['predictions']['Q2'],
            })

            metrics = calculate_performance_metrics(predictions_df)

            print("\nModel performance metrics:")
            for quantity, quantity_metrics in metrics.items():
                print(f"\n{quantity}:")
                for metric_name, metric_value in quantity_metrics.items():
                    print(f"  {metric_name}: {metric_value:.6f}")

        print("\nStep 5: Saving fitting results...")
        output_file = os.path.join(BASE_DIR, OUTPUT_GA_RESULTS)

        with pd.ExcelWriter(output_file, engine="openpyxl") as writer:
            if 'predictions' in results:
                predictions_df.to_excel(writer, sheet_name=FILE_CONFIG['excel_sheets']['predictions'], index=False)

            params_df = pd.DataFrame({
                "Parameter": PARAM_NAMES,
                "Best Value": optimizer.best_params,
                "Lower Bound": [bound[0] for bound in BOUNDS],
                "Upper Bound": [bound[1] for bound in BOUNDS],
                "Description": [bound[3] for bound in PARAMETER_BOUNDS],
            })
            params_df.to_excel(writer, sheet_name=FILE_CONFIG['excel_sheets']['parameters'], index=False)

            if 'predictions' in results:
                metrics_data = []
                for quantity, quantity_metrics in metrics.items():
                    for metric_name, metric_value in quantity_metrics.items():
                        metrics_data.append({
                            "Quantity": quantity,
                            "Metric": metric_name,
                            "Value": metric_value
                        })
                metrics_df = pd.DataFrame(metrics_data)
                metrics_df.to_excel(writer, sheet_name=FILE_CONFIG['excel_sheets']['performance'], index=False)

            if results['history']:
                history_df = pd.DataFrame({
                    "Iteration": range(1, len(results['history']) + 1),
                    "Fitness": [h['fitness'] for h in results['history']],
                    "Time": [h['time'] for h in results['history']],
                })
                history_df.to_excel(writer, sheet_name=FILE_CONFIG['excel_sheets']['convergence'], index=False)

            config_data = []
            for section_name, section_config in [
                ('GPU Config', GPU_CONFIG),
                ('Optimization Config', OPTIMIZATION_CONFIG),
                ('Physics Config', PHYSICS_CONFIG),
                ('Fitting Weights', FITTING_WEIGHTS),
                ('Prediction Config', PREDICTION_CONFIG),
                ('File Config', FILE_CONFIG),
                ('Debug Config', DEBUG_CONFIG)
            ]:
                for key, value in section_config.items():
                    config_data.append({
                        'Section': section_name,
                        'Parameter': key,
                        'Value': str(value),
                        'Type': type(value).__name__
                    })

            config_df = pd.DataFrame(config_data)
            config_df.to_excel(writer, sheet_name=FILE_CONFIG['excel_sheets']['configuration'], index=False)

        print(f"Fitting results saved to: {output_file}")

        print("\nStep 6: Generating visualization plots...")
        plot_file = os.path.join(BASE_DIR, OUTPUT_GA_PLOT)

        if 'predictions' in results:
            create_fitting_plots(predictions_df, plot_file)
        else:
            create_results_plot(results, plot_file)

        print(f"Plots saved to: {plot_file}")

        print("\nStep 7: Processing prediction conditions...")
        prediction_df = read_prediction_conditions()

        if prediction_df is not None:
            print(f"Predicting {len(prediction_df)} conditions...")
            prediction_results = optimizer.predict_conditions(prediction_df)

            prediction_file = os.path.join(BASE_DIR, OUTPUT_PREDICTION_RESULTS)
            with pd.ExcelWriter(prediction_file, engine="openpyxl") as writer:
                prediction_results.to_excel(writer, sheet_name="Predictions", index=False)

                try:
                    organized_data = organize_predictions_by_parameter(
                        prediction_results, PREDICTION_CONFIG['sheet_parameter']
                    )

                    for sheet_name, pivot_table in organized_data.items():
                        pivot_table.to_excel(writer, sheet_name=sheet_name)

                    print(f"Predictions organized by {PREDICTION_CONFIG['sheet_parameter']}")

                except Exception as e:
                    print(f"Error organizing predictions: {e}")

            print(f"Predictions saved to: {prediction_file}")

            print("\nPredicting for input data...")
            if 'predictions' in results:
                input_prediction_file = os.path.join(BASE_DIR, OUTPUT_INPUT_PREDICTIONS)

                with pd.ExcelWriter(input_prediction_file, engine="openpyxl") as writer:
                    predictions_df.to_excel(writer, sheet_name="Input Data Predictions", index=False)

                print(f"Input data predictions saved to: {input_prediction_file}")

        else:
            print("Skipping predictions (no prediction conditions file found)")

        print("\n" + "=" * 80)
        print("GPU-Accelerated Dynamic Recrystallization Analysis Complete!")
        print("=" * 80)

        print(f"\nAnalysis Summary:")
        print(f"  Best fitness: {optimizer.best_fitness:.8f}")
        print(f"  Optimization iterations: {len(results['history'])}")
        print(f"  Parameters: {len(PARAM_NAMES)} (using optimized activation energy formula)")

        if 'predictions' in results:
            print(f"\nModel Performance:")
            for quantity, quantity_metrics in metrics.items():
                r2 = quantity_metrics.get("R2", 0)
                rmse = quantity_metrics.get("RMSE", 0)
                print(f"  {quantity}: R = {r2:.4f}, RMSE = {rmse:.6f}")

        if DEBUG_CONFIG['monitor_computation_time']:
            print(f"\nPerformance Statistics:")
            print(f"  Total time: {total_time:.2f} seconds")
            print(f"  Data points: {n_points}")
            print(f"  Generations: {len(results['history'])}")
            print(f"  Est. speedup: ~{total_time * 15:.1f}s (CPU) vs {total_time:.1f}s (GPU)")

        print(f"\nOutput Files:")
        print(f"  Fitting results: {OUTPUT_GA_RESULTS}")
        print(f"  Fitting plots: {OUTPUT_GA_PLOT}")
        if prediction_df is not None:
            print(f"  Predictions: {OUTPUT_PREDICTION_RESULTS}")
            print(f"  Input predictions: {OUTPUT_INPUT_PREDICTIONS}")

        print(f"\nOptimized Activation Energy Formula:")
        print(f"  Q1 = Q01 + alpha1/T + beta1*ln(strain_rate)")
        print(f"  Q2 = Q02 + alpha2/T + beta2*ln(strain_rate)")

        print(f"\nGPU Acceleration Features:")
        print(f"  Parallel computing: Batch fitness evaluation")
        print(f"  Mixed precision: Enhanced computation speed")
        print(f"  Memory optimization: Dynamic batch adjustment")

        return optimizer, results

    except Exception as e:
        print(f"\nError during analysis: {e}")
        print(f"Error type: {type(e).__name__}")
        print("Detailed error trace:")
        import traceback
        traceback.print_exc()
        return None, None

if __name__ == "__main__":
    print("Starting GPU-Accelerated Dynamic Recrystallization Analysis System...")

    if not check_gpu_status():
        print("GPU check failed! Please resolve GPU/CUDA issues first")
        input("Press Enter to exit...")
    else:
        try:
            success_result = main()
            optimizer, results = success_result if success_result else (None, None)

            if optimizer is not None:
                print(f"\nOptimization successful!")

                if results and 'best_params' in results:
                    print(f"\nPrediction example:")
                    try:
                        test_conditions = pd.DataFrame([{
                            "strain": 0.3,
                            "temperature": 1000,
                            "rate": 1.0
                        }])

                        test_pred = optimizer.predict_conditions(test_conditions)
                        x_drx_pred = test_pred['X_drx_predicted'].iloc[0]

                        print(f"  Conditions: T=1000C, rate=1.0s^-1, strain=0.3")
                        print(f"  Predicted X_drx: {x_drx_pred:.3f}")

                    except Exception as e:
                        print(f"  Prediction failed: {e}")
                        import traceback
                        traceback.print_exc()

                print(f"\nGPU-Accelerated Dynamic Recrystallization Analysis System completed successfully!")

            else:
                print(f"\nOptimization failed!")

        except KeyboardInterrupt:
            print("\nProgram interrupted by user")
        except Exception as e:
            print(f"\nProgram terminated with error: {e}")
            import traceback
            traceback.print_exc()

    input(f"\nPress Enter to exit...")